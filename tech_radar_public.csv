name,ring,quadrant,isNew,description
The streaming data warehouse,Assess,Techniques,TRUE,"<p>The need to respond quickly to customer insights has driven increasing adoption of event-driven architectures and stream processing. Frameworks such as <a href=""/radar/platforms/apache-spark"">Spark</a>, <a href=""/radar/platforms/apache-flink"">Flink</a> or <a href=""/radar/platforms/kafka-streams"">Kafka Streams</a> offer a paradigm where simple event consumers and producers can cooperate in complex networks to deliver real-time insights. But this programming style takes time and effort to master and when implemented as single-point applications, it lacks interoperability. Making stream processing work universally on a large scale can require a significant engineering investment. Now, a new crop of tools is emerging that offers the benefits of stream processing to a wider, established group of developers who are comfortable using SQL to implement analytics. Standardizing on SQL as the universal streaming language lowers the barrier for implementing streaming data applications. Tools like <a href=""/radar/languages-and-frameworks/ksqldb"">ksqlDB</a> and <a href=""/radar/platforms/materialize"">Materialize</a> help transform these separate applications into unified platforms. Taken together, a collection of SQL-based streaming applications across an enterprise might constitute a <strong>streaming data warehouse</strong>.</p>"
TinyML,Assess,Techniques,TRUE,"<p>Until recently, executing a machine-learning (ML) model was seen as computationally expensive and in some cases required special-purpose hardware. While creating the models still broadly sits within this classification, they can be created in a way that allows them to be run on small, low-cost and low-power consumption devices. This technique, called <strong><a href=""https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79"">TinyML</a></strong>, has opened up the possibility of running ML models in situations many might assume infeasible. For example, on battery-powered devices, or in disconnected environments with limited or patchy connectivity, the model can be run locally without prohibitive cost. If you've been considering using ML but thought it unrealistic because of compute or network constraints, then this technique is worth assessing.</p>"
Azure Data Factory for orchestration,Hold,Techniques,FALSE,"<p>For organizations using Azure as their primary cloud provider, <a href=""https://azure.microsoft.com/en-us/services/data-factory/"">Azure Data Factory</a> is currently the default for orchestrating data-processing pipelines. It supports data ingestion, copying data from and to different storage types on prem or on Azure and executing transformation logic. Although we've had adequate experience with Azure Data Factory for simple migrations of data stores from on prem to the cloud, we discourage the use of <strong>Azure Data Factory for orchestration</strong> of complex data-processing pipelines and workflows. We've had some success with Azure Data Factory when it's used primarily to move data between systems. For more complex data pipelines, it still has its challenges, including poor debuggability and error reporting; limited observability as Azure Data Factory logging capabilities don't integrate with other products such as Azure Data Lake Storage or Databricks, making it difficult to get an end-to-end observability in place; and availability of data source-triggering mechanisms only to certain regions. At this time, we encourage using other open-source orchestration tools (e.g., <a href=""/radar/tools/airflow"">Airflow</a>) for complex data pipelines and limiting Azure Data Factory for data copying or snapshotting. Our teams continue to use Data Factory to move and extract data, but for larger operations we recommend other, more well-rounded workflow tools.</p>"
Production data in test environments,Hold,Techniques,FALSE,"<p>We continue to perceive <strong>production data in test environments</strong> as an area for concern. Firstly, many examples of this have resulted in reputational damage, for example, where an incorrect alert has been sent from a test system to an entire client population. Secondly, the level of security, specifically around protection of private data, tends to be less for test systems. There is little point in having elaborate controls around access to production data if that data is copied to a test database that can be accessed by every developer and QA. Although you <em>can</em> obfuscate the data, this tends to be applied only to specific fields, for example, credit card numbers. Finally, copying production data to test systems can break privacy laws, for example, where test systems are hosted or accessed from a different country or region. This last scenario is especially problematic with complex cloud deployments. Fake data is a safer approach, and tools exist to help in its creation. We do recognize there are reasons for <em>specific</em> elements of production data to be copied, for example, in the reproduction of bugs or for training of specific ML models. Here our advice is to proceed with caution.</p>"
Azure DevOps,Adopt,Platforms,FALSE,"<p>As the <strong><a href=""https://azure.microsoft.com/en-us/services/devops/"">Azure DevOps</a></strong> ecosystem keeps growing, our teams are using it more with success. These services contain a set of managed services, including hosted Git repos, build and deployment pipelines, automated testing tooling, backlog management tooling and artifact repository. We've seen our teams gaining experience in using this platform with good results, which means Azure DevOps is maturing. We particularly like its flexibility; it allows you to use the services you want even if they're from different providers. For instance, you could use an external Git repository while still using the Azure DevOps pipeline services. Our teams are especially excited about <a href=""https://azure.microsoft.com/en-us/services/devops/pipelines/"">Azure DevOps Pipelines</a>. As the ecosystem matures, we're seeing an uptick in onboarding teams that are already on the Azure stack as it easily integrates with the rest of the Microsoft world.</p>"
Azure Pipeline templates,Trial,Platforms,TRUE,"<p><strong><a href=""https://docs.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops"">Azure Pipeline templates</a></strong> allow you to remove duplication in your Azure Pipeline definition through two mechanisms. With ""includes"" templates, you can reference a template such that it will expand inline like a parameterized C++ macro, allowing a simple way of factoring out common configuration across stages, jobs and steps. With ""extends"" templates, you can define an outer shell with common pipeline configuration, and with the <a href=""https://docs.microsoft.com/en-us/azure/devops/pipelines/process/approvals?view=azure-devops&tabs=check-pass#required-template"">required template approval</a>, you can fail the build if the pipeline doesn't extend certain templates, preventing malicious attacks against the pipeline configuration itself. Along with <a href=""/radar/platforms/circleci"">CircleCI</a> Orbs and the newer <a href=""/radar/platforms/reusable-workflows-in-github-actions"">GitHub Actions Reusable Workflows</a>, Azure Pipeline templates are part of the trend of creating modularity in pipeline design across multiple platforms, and several of our teams have been happy using them.</p>"
Google BigQuery ML,Adopt,Platforms,FALSE,"<p>Since we last blipped about <strong><a href=""https://cloud.google.com/bigquery-ml/docs"">Google BigQuery ML</a></strong>, more sophisticated models such as Deep Neural Networks and AutoML Tables have been added by connecting BigQuery ML with TensorFlow and Vertex AI as its backend. BigQuery has also introduced support for time series forecasting. One of our concerns previously was <a href=""/radar/techniques/explainability-as-a-first-class-model-selection-criterion"">explainability</a>. Earlier this year, <a href=""https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-xai-overview"">BigQuery Explainable AI</a> was announced for general availability, taking a step in addressing this. We can also export BigQuery ML models to Cloud Storage as a Tensorflow SavedModel and use them for online prediction. There remain trade-offs like ease of ""continuous delivery for machine learning"" but with its low barrier to entry, BigQuery ML remains an attractive option, particularly when the data already resides in BigQuery.</p>"
Google Cloud Dataflow,Adopt,Platforms,FALSE,"<p><strong><a href=""https://cloud.google.com/dataflow/"">Google Cloud Dataflow</a></strong> is a cloud-based data-processing service for both batch and real-time data-streaming applications. Our teams are using Dataflow to create processing pipelines for integrating, preparing and analyzing large data sets, with <a href=""https://beam.apache.org/"">Apache Beam</a>'s unified programming model on top to ease manageability. We first featured Dataflow in 2018, and its stability, performance and rich feature set make us confident to move it to Trial in this edition of the Radar.</p>"
VerneMQ,Assess,Platforms,TRUE,"<p><strong><a href=""https://github.com/vernemq/vernemq"">VerneMQ</a></strong> is an open-source, high-performance, distributed MQTT broker. We've blipped other MQTT brokers in the past like <a href=""/radar/platforms/mosquitto"">Mosquitto</a> and <a href=""/radar/platforms/emq"">EMQ</a>. Like EMQ and RabbitMQ, VerneMQ is also based on Erlang/OTP which makes it highly scalable. It scales horizontally and vertically on commodity hardware to support a high number of concurrent publishers and consumers while maintaining low latency and fault tolerance. In our internal benchmarks, we've been able to achieve a few million concurrent connections in a single cluster. While it's not new, we've used it in production for some time now, and it has worked well for us.</p>"
Apache Iceberg,Assess,Platforms,TRUE,"<p><strong><a href=""https://iceberg.apache.org/"">Apache Iceberg</a></strong> is an open table format for very large analytic data sets. Iceberg supports modern analytical data operations such as record-level insert, update, delete, <a href=""https://iceberg.apache.org/docs/latest/spark-queries/#time-travel"">time-travel queries</a>, ACID transactions, <a href=""https://iceberg.apache.org/docs/latest/partitioning/#icebergs-hidden-partitioning"">hidden partitioning</a> and <a href=""https://iceberg.apache.org/docs/latest/evolution/"">full schema evolution</a>. It supports multiple underlying file storage formats such as <a href=""https://parquet.apache.org/"">Apache Parquet</a>, <a href=""https://orc.apache.org/"">Apache ORC</a> and <a href=""https://avro.apache.org/docs/1.2.0/"">Apache Avro</a>. Many data-processing engines support Apache Iceberg, including SQL engines such as <a href=""https://www.dremio.com/"">Dremio</a> and <a href=""https://trino.io/"">Trino</a> as well as (structured) streaming engines such as <a href=""https://spark.apache.org/"">Apache Spark</a> and <a href=""https://flink.apache.org/"">Apache Flink</a>.</p>
 
 <p>Apache Iceberg falls in the same category as <a href=""https://delta.io/"">Delta Lake</a> and <a href=""https://hudi.apache.org/"">Apache Hudi</a>. They all more or less support similar features, but each differs in the underlying implementations and detailed feature lists. Iceberg is an independent format and is not native to any specific processing engine, hence it's supported by an increasing number of platforms, including <a href=""https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html"">AWS Athena</a> and <a href=""https://www.snowflake.com/"">Snowflake</a>. For the same reason, Apache Iceberg, unlike native formats such as Delta Lake, may not benefit from optimizations when used with Spark.</p>"
Colima,Assess,Platforms,TRUE,"<p><strong><a href=""https://github.com/abiosoft/colima"">Colima</a></strong> is becoming a popular open alternative to Docker for Desktop. It provisions the Docker container runtime in a Lima VM, configures the Docker CLI on macOS and handles port-forwarding and volume mounts. Colima uses <a href=""https://containerd.io/"">containerd</a> as runtime, which is also the runtime on most managed Kubernetes services (thus improved dev-prod parity). With Colima you can easily use and test the latest features of containerd, such as lazy loading for container images. With its good performance, we're watching Colima as a strong potential for the open-source choice alternative to Docker for Desktop.</p>"
Collibra,Assess,Platforms,TRUE,"<p>In the increasingly crowded space that is the enterprise data catalog market, our teams have enjoyed working with <strong><a href=""https://www.collibra.com/us/en"">Collibra</a></strong>. They liked the deployment flexibility of either a SaaS or self-hosted instance, the wide range of functionality included out of the box, including data governance, lineage, quality and observability. Users also have the option to use a smaller subset of capabilities required by a more decentralized approach such as a <a href=""/radar/techniques/data-mesh"">data mesh</a>. The real feather in its cap has been their often overlooked customer support, which our people have found to be collaborative and supportive. Of course, there's a tension between simple data catalogs and more full featured enterprise platforms, but so far the teams using it are happy with how Collibra has supported their needs.</p>"
CycloneDX,Assess,Platforms,TRUE,"<p><strong><a href=""https://cyclonedx.org/"">CycloneDX</a></strong> is a standard for describing a machine-readable <a href=""/radar/techniques/software-bill-of-materials"">Software Bill of Materials</a> (SBOM). As software and compute fabrics increase in complexity, <em>software</em> becomes harder to define. Originating with OWASP, CycloneDX improves on the older SPDX standard with a broader definition that extends beyond the local machine dependencies to include runtime service dependencies. You'll also find implementations in several languages, an <a href=""https://cyclonedx.org/tool-center/"">ecosystem</a> of supporting integrations and a <a href=""https://github.com/CycloneDX/cyclonedx-cli"">CLI tool</a> that lets you analyze and change SBOMs with appropriate signing and verification.</p>"
AKHQ,Assess,Tools,TRUE,"<p><strong><a href=""https://akhq.io/docs/#installation"">AKHQ</a></strong> is a GUI for Apache Kafka that lets you manage topics, topics data, consumer groups and more. Some of our teams have found AKHQ to be an effective tool to watch the real-time status of a Kafka cluster. You can, for example, browse the topics on a cluster. For each topic, you can visualize the name, the number of messages stored, the disk size used, the time of the last record, the number of partitions, the replication factor with the in-sync quantity and the consumer group. With options for Avro and Protobuf deserialization, AKHQ can help you understand the flow of data in your Kafka environment.</p>"
Metaflow,Assess,Tools,TRUE,"<p><strong><a href=""https://github.com/Netflix/metaflow"">Metaflow</a></strong> is a user-friendly Python library and back-end service that helps data scientists and engineers build and manage production-ready data processing, ML training and inference workflows. Metaflow provides Python APIs that structure the code as a directed graph of steps. Each step can be decorated with flexible configurations such as the required compute and storage resources. Code and data artifacts for each step's run (aka task) are stored and can be retrieved either for future runs or the next steps in the flow, enabling you to recover from errors, repeat runs and track versions of models and their dependencies across multiple runs.</p>
 
 <p>The value proposition of Metaflow is the simplicity of its idiomatic Python library: it fully integrates with the build and run-time infrastructure to enable running data engineering and science tasks in local and scaled production environments. At the time of writing, Metaflow is heavily integrated with AWS services such as S3 for its data store service and step functions for orchestration. Metaflow supports R in addition to Python. Its core features are open sourced.</p>
 
 <p>If you're building and deploying your production ML and data-processing pipelines on AWS, Metaflow is a lightweight full-stack alternative framework to more complex platforms such as <a href=""/radar/tools/mlflow"">MLflow</a>.</p>"
SQLFluff,Assess,Tools,TRUE,"<p>While linting is an ancient practice in the software world, it's had slower adoption in the data world. <strong><a href=""https://docs.sqlfluff.com/en/stable/"">SQLFluff</a></strong> is a cross-dialect SQL linter written in Python that ships with a simple command line interface (CLI), making it easy to incorporate into a CI/CD pipeline. If you're comfortable with the default conventions, then SQLFluff works without any additional configuration after installing it and will enforce a strongly opinionated set of formatting standards; setting your own conventions involves adding a configuration dotfile. The CLI can automatically fix certain classes of violations that involve formatting concerns like whitespace or uppercasing of keywords. SQLFluff is still new, but we're excited to see SQL getting some attention in the linting world.</p>"
Typesense,Assess,Tools,TRUE,"<p><strong><a href=""https://github.com/typesense/typesense"">Typesense</a></strong> is a fast, typo-tolerant text search engine. For use cases with large volumes of data, Elasticsearch might still be a good option as it provides a horizontally scalable disk-based search solution. However, if you're building a latency-sensitive search application with a search index size that can fit in memory, Typesense is a powerful alternative and another option to evaluate alongside tools such as <a href=""/radar/platforms/meilisearch"">Meilisearch</a>.</p>"
Java 17,Assess,languages-and-frameworks,TRUE,"<p>We don't routinely feature new versions of languages, but we wanted to highlight the new long-term support (LTS) version of Java, version 17. While there are promising new features, such as the preview of <a href=""https://openjdk.java.net/jeps/406"">pattern matching</a>, it's the switch to the new LTS process that should interest many organizations. We recommend organizations assess new releases of Java as and when they become available, making sure they adopt new features and versions as appropriate. Surprisingly many organizations do not routinely adopt newer versions of languages even though regular updates help keep things small and manageable. Hopefully the new LTS process, alongside organizations moving to regular updates, will help avoid the ""too expensive to update"" trap that ends with production software running on an end-of-life version of Java.</p>"
MistQL,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://github.com/evinism/mistql"">MistQL</a></strong> is a small domain-specific language for performing computations on JSON-like structures. Originally built for handcrafted feature extraction of machine-learning models on the frontend, MistQL currently supports a JavaScript implementation for browsers and a Python implementation for server-side use cases. We quite like its clean composable functional syntax, and we encourage you to assess it based on your needs.</p>"
ShedLock,Assess,languages-and-frameworks,TRUE,"<p>Executing a scheduled task once and only once in a cluster of distributed processors is a relatively common requirement. For example, the situation might arise when ingesting a batch of data, sending a notification or performing some regular cleanup activity. But this is a notoriously difficult problem. How does a group of processes cooperate reliably over laggy and less reliable networks? Some kind of locking mechanism is required to coordinate actions across the cluster. Fortunately, a variety of distributed stores can implement a lock. Systems like <a href=""https://zookeeper.apache.org/"">ZooKeeper</a> and <a href=""/radar/tools/consul"">Consul</a> as well as databases such as DynamoDB or <a href=""/radar/platforms/couchbase"">Couchbase</a> have the necessary underlying mechanisms to manage consensus across the cluster. <strong><a href=""https://github.com/lukas-krecan/ShedLock"">ShedLock</a></strong> is a small library for taking advantage of these providers in your own Java code, if you're looking to implement your own scheduled tasks. It provides an API for acquiring and releasing locks as well as connectors to a wide variety of lock providers. If you're writing your own distributed tasks but don't want to take on the complexity of an entire orchestration platform like <a href=""/radar/platforms/kubernetes"">Kubernetes</a>, ShedLock is worth a look.</p>"
SpiceDB,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://github.com/authzed/spicedb"">SpiceDB</a></strong> is a database system, inspired by Google's <a href=""https://research.google/pubs/pub48190"">Zanzibar</a>, for managing application permissions. With SpiceDB, you create a schema to model the permissions requirements and use the <a href=""https://docs.authzed.com/reference/api#client-libraries"">client library</a> to apply the schema to one of the <a href=""https://docs.authzed.com/spicedb/selecting-a-datastore"">supported databases</a>, insert data and query to efficiently answer questions like ""Does this user have access to this resource?"" or even the inverse ""What are all the resources this user has access to?"" We usually advocate separating the authorization policies from code, but SpiceDB takes it a step further by separating data from the policy and storing it as a graph to efficiently answer authorization queries. Because of this separation, you have to ensure that the changes in your application's primary data store are reflected in SpiceDB. Among other Zanzibar-inspired implementations, we find SpiceDB to be an interesting framework to assess for your authorization needs.</p>"
sqlc,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://github.com/kyleconroy/sqlc"">sqlc</a></strong> is a compiler that generates type-safe idiomatic Go code from SQL. Unlike other approaches based on object-relational mapping (ORM), you continue to write plain SQL for your needs. Once invoked, sqlc checks the correctness of the SQL and generates performant Go code, which can be directly called from the rest of the application. With stable support for both PostgreSQL and MySQL, sqlc is worth a look, and we encourage you to assess it.</p>"
Python,Adopt,languages-and-frameworks,,
Numpy,,,,